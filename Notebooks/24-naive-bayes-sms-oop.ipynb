{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9895dade-233c-4960-ad68-b8b1d1e77bd7",
   "metadata": {},
   "source": [
    "# Naive Bayes - Object Oriented (OOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fd255c5-8e45-4b98-9d5f-4d5b52df8654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.api.types import is_string_dtype\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b05c520f-e44a-43a2-9755-d252609ed0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "    def __init__(self):\n",
    "        self.df_p = None\n",
    "        self.p_spam = None\n",
    "        self.p_ham = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = self.__prepare_data(X)\n",
    "        self.df_p = self.__calculate_df_p(X, y)\n",
    "        self.p_spam, self.p_ham = self__calculate_p_classes(y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = self.__prepare_data(X)\n",
    "        df_X = X.values.tolist()\n",
    "        query = df_X[0][1]\n",
    "        query_words = query.split(' ')\n",
    "        \n",
    "        p_words_ham = 1\n",
    "        p_words_spam = 1\n",
    "\n",
    "        for word in query_words:\n",
    "            df_p_word = df_p[df_p[\"word\"] == word]\n",
    "\n",
    "            p_words_ham *= df_p_word['prob_ham'].values[0]\n",
    "            p_words_spam *= df_p_word['prob_spam'].values[0]\n",
    "\n",
    "        p_words = (p_ham * p_words_ham) + (p_spam * p_words_spam)\n",
    "\n",
    "        p_final_ham = p_words_ham * p_ham / p_words\n",
    "        p_final_spam = p_words_spam * p_spam / p_words\n",
    "\n",
    "        print(f\"Ham: {p_final_ham:.5f}, Spam: {p_final_spam:.5f}\")\n",
    "\n",
    "        if p_final_ham > p_final_spam:\n",
    "            classified = 'ham'\n",
    "        else:\n",
    "            classified = 'spam'\n",
    "            \n",
    "        return classified\n",
    "        \n",
    "        \n",
    "    def __prepare_data(self, X):\n",
    "        X = self.__lower_case(X)\n",
    "        X = self.__remove_special(X)\n",
    "        X = self.__apply_nlp(X)\n",
    "        X = self.__merge_columns(X)\n",
    "        \n",
    "        \n",
    "    def __merge_columns(self, X):\n",
    "        for column in X.columns:\n",
    "            # '_nbc' as NaiveBayesClassifier\n",
    "            \n",
    "            X[\"_nbc\"] = X[col] + ' '\n",
    "        return X\n",
    "    \n",
    "    def __words_count(self, X, y):\n",
    "        for column in X.columns:\n",
    "            if is_string_dtype(X.dtypes[column]):\n",
    "                df_count = X.set_index(y)['_nbc'].str.split().explode().reset_index().groupby(['sms', 'y']).size().to_frame()\n",
    "                df_count = df_count.reset_index()\n",
    "                return df_count\n",
    "\n",
    "    def __calculate_p_classes(self, y):\n",
    "        df_general = y.groupby(y.columns).count().reset_index()\n",
    "\n",
    "        total_spam = df_general.loc[df_general[y.columns] == 'spam', '_nbc'].values[0]\n",
    "        total_ham = df_general.loc[df_general[y.columns] == 'ham', '_nbc'].values[0]\n",
    "        self.p_spam = total_spam / df.shape[0]\n",
    "        self.p_ham = total_ham / df.shape[0]\n",
    "        \n",
    "        return self.p_spam, self.p_ham\n",
    "\n",
    "    def __calculate_df_p(self, X, y):\n",
    "        columns=['word', 'prob_ham', 'prob_spam']\n",
    "        df_p = pd.DataFrame(data=None, columns=columns)\n",
    "        \n",
    "        words = self.__get_words_list(X)\n",
    "        df_count = self.__words_count(X, y)\n",
    "        \n",
    "        for word in words:\n",
    "            entry = []\n",
    "            df_word = df_count[df_count[\"_nbc\"] == word]\n",
    "            if df_word.shape[0] > 0:\n",
    "                count_word_spam = df_word.loc[df_word[y.columns] == 'spam', 0].values\n",
    "                count_word_ham = df_word.loc[df_word[y.columns] == 'ham', 0].values\n",
    "\n",
    "                if len(count_word_spam) > 0:\n",
    "                    count_word_spam = count_word_spam[0]\n",
    "                else:\n",
    "                    count_word_spam = 1\n",
    "\n",
    "                if len(count_word_ham) > 0:\n",
    "                    count_word_ham = count_word_ham[0]\n",
    "                else:\n",
    "                    count_word_ham = 1\n",
    "\n",
    "                p_word_ham = count_word_ham / total_ham\n",
    "                p_word_spam = count_word_spam / total_spam\n",
    "\n",
    "\n",
    "                entry = [word, p_word_ham, p_word_spam]\n",
    "\n",
    "                df_entry = pd.DataFrame(data=[entry], columns=columns)\n",
    "                if df_p.shape[0] > 0:\n",
    "                    df_p = pd.concat([df_p, df_entry])\n",
    "                else:\n",
    "                    df_p = df_entry\n",
    "\n",
    "        self.df_p = df_p.reset_index(drop=True)\n",
    "        return self.df_p\n",
    "        \n",
    "    def __lower_case(self, X):\n",
    "        df = X.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "        return df\n",
    "    \n",
    "    def __remove_special(self, X):\n",
    "        for column in X.columns:\n",
    "            if is_string_dtype(X.dtypes[column]):\n",
    "                X[column] = X[column].str.replace(\"[^a-z 0-9]+\", \"\", regex=True)\n",
    "        return X\n",
    "    \n",
    "    def __apply_nlp(self, X):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stemmer = PorterStemmer()\n",
    "\n",
    "        def preprocess(sentence):\n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "            filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "            stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "            lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "            return \" \".join(lemma_words)\n",
    "\n",
    "        for column in X.columns:\n",
    "            if is_string_dtype(X.dtypes[column]):\n",
    "                X[column] = X[column].map(lambda s:preprocess(s)) \n",
    "    \n",
    "        return X\n",
    "    \n",
    "    def __get_words_list(self, X):\n",
    "        words = []\n",
    "        for column in X.columns:\n",
    "            if is_string_dtype(X.dtypes[column]):\n",
    "                words.append(dX[column].str.split().explode().drop_duplicates().values)\n",
    "        \n",
    "        words = list(set(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801013dc-0939-4608-b85d-5a6b49bb311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveBayes()\n",
    "model.fit(X, y)\n",
    "prediction = model.predict(X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
